---
title: "Project 2: Unveiling Ecological Dynamics Through Simulation and Visualization of Biodiversity Data Cubes"
subtitle: "Setup and introduction"
author: "Ward Langeraert"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  chunk_output_type: console
---

```{r setup, message=FALSE, warning=FALSE}
# Setup
library(knitr)
library(here)
opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE)
opts_knit$set(root.dir = here())

# Packages
library(tidyverse)
library(sf)
```

# Introduction

Ecological systems are inherently variable, and a single dataset does not capture the full range of possible conditions. *Simulations* allow us to explore a broader *spectrum of scenarios*, considering different combinations of *parameters*. As far as we know, there is no simulation framework present for *biodiversity data cubes*. A simulation framework in this context would help to examine the generalisability of the (statistical) properties of these data cubes and the derived indicators.

# Technical setup and requirements
## Software installation

To join this project there are three software requirements: R, RStudio and Git.
R is a programming language and environment specifically designed for statistical computing and data analysis.
RStudio is an integrated development environment (IDE) for R that makes working with R more accessible and efficient.
Git is a distributed version control system used for tracking changes in source code during software development.
You can download R and RStudio from [here](https://posit.co/download/rstudio-desktop/) and Git from [here](https://git-scm.com/downloads).

## Setting up version control for efficient collaboration

If you do not have a GitHub account, you can sign up [for a new GitHub account](https://docs.github.com/en/get-started/start-your-journey/creating-an-account-on-github). 
If this is not yet done, make sure to [connect RStudio to Git and GitHub](https://happygitwithr.com/rstudio-git-github).
Clone the hackathon GitHub repository [hackathon-projects-2024](https://github.com/b-cubed-eu/hackathon-projects-2024) via RStudio.

# Monte Carlo simulation framework for biodiversity data cubes

In this project, we will aim to create a simulation framework for biodiversity data cubes based on [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method). 

We need to simulate three different processes related to different variables that depend on *species*, *observation*, *space* and *time*.

1.  occurrence process
2.  detection process
3.  grid designation process

In this project, we will focus on the occurrence and detection processes. For grid designation, R code is already available.

```{r processes-tab, echo=FALSE}
tibble(
  process = c(
    rep("occurrence", 2),
    rep("detection", 3)
  ),
  variable = c(
    "rarity",
    "spatial clustering",
    "detection probability",
    "sampling effort",
    "spatial uncertainty"
  ),
  dependency = c(
    "species, time",
    "species",
    "species",
    "space, time",
    "observation"
  )) %>%
  kable()
```

```{r visual-framework, out.width="80%", fig.align="center", echo=FALSE}
include_graphics(here("media", "visual_proposal.png"), error = FALSE)
```

## Occurrence process

To simulate the occurrence of species in a certain spatial region, we need information on the rarity as well as a measure for spatial clustering.

### Rarity

-  Rarity can be different for each *species* and can change over *time*.
-  Values related to abundance/density
-  Implementation
   - Species: abundance/density for each species. Density can be converted to abundance based on the area of the polygon. Abundances can be generated from the Poisson distribution (`rpois()`)
   - Time: the order of random walk or a function (e.g. sinus, linear, exponential …)

```{r abundance-sim}
# Set seed for reproducibility
set.seed(123)

# Number of time points
n_time_points <- 50

# Time vector
time <- 1:n_time_points

# Sinusoidal trend parameters
amplitude <- 10
frequency <- 0.1

# Generate sinusoidal trend
sinusoidal_trend <- amplitude * sin(2 * pi * frequency * time)

# Poisson distribution parameters
lambda <- 100 - 1*time

# Number of simulations
n_sim <- 100
list_abundances <- vector("list", length = n_sim)

for (i in seq_len(n_sim)) {
  # Simulate abundances with Poisson distribution and sinusoidal trend
  abundances <- rpois(n_time_points, lambda + sinusoidal_trend)
  
  # Create a data frame
  list_abundances[[i]] <- data.frame(time = time, abundance = abundances,
                                     sim = i)
}
data_abundances <- do.call(rbind.data.frame, list_abundances)

# Plot the simulated abundances over time using ggplot2
ggplot(data_abundances, aes(x = time, y = abundance, colour = factor(sim))) +
  geom_line() +
  labs(x = "Time", y = "Species abundance",
       title = paste(n_sim, "simulated abundances with linear decreasing",
                     "sinusoidal trend over time")) +
  scale_y_continuous(limits = c(0, NA)) +
  theme_minimal() +
  theme(legend.position = "")
```

### Spatial clustering

-  Spatial clustering can be different for each *species*.
-  Values related to clustering ([Moran’s I](https://en.wikipedia.org/wiki/Moran%27s_I)? value between -1 (regular) to 0 (random) to 1 (clustered))
-  Implementation
   - Species: Get number of coordinate pairs based on clustering value equal to the abundance of the species.
   - Clustering value = Moran’s I? `runif(n, min = -1, max = 1)` for uniform distribution between -1 and 1 or the Beta distribution to get a skewed distribution between -1 and 1. Let $X \sim Beta(\alpha, \beta)$, then then the transformation $Y = 2X − 1$ will result in a distribution with values between -1 and 1.

```{r clustering-beta}
# Generate data
n <- 100
x_values <- seq(0, 1, length.out = n)

# Parameters Beta distribution
alpha <- c(0.5, 2, 2, 2, 7, 8)
beta <- c(0.5, 1, 2, 7, 2, 8)

# Calculate Beta densities in dataframe
clustering_df <- tibble(
  alpha = alpha,
  beta = beta) %>%
  expand(nesting(alpha, beta), x_values) %>%
  rowwise() %>%
  mutate(
    density = dbeta(x_values, shape1 = alpha, shape2 = beta),
    y_values = 2 * x_values - 1,
    distribution = paste0("Beta(", alpha, ",", beta, ")")
  )

# Plot the densities
ggplot(clustering_df, aes(x = y_values, y = density)) +
  geom_line() +
  labs(x = "Moran's I", y = "Density",
       title = "Densities of different Beta distributions") +
  facet_wrap(~distribution) +
  theme(legend.position = "") +
  theme_minimal()
```

## Detection process

To simulate the detection of individuals in a certain spatial region, we need information on the detection probability	of each species and sampling effort. The probability of detecting an individual can be the product of detection probability and sampling probability. Once observed, there is also always a certain spatial uncertainty related to each observation (`coordinateUncertaintyInMeters`).

### Detection probability
-  Detection probability can be different for each *species*.
-  Values are a probability between 0 (never detected) to 1 (always detected)
-  Implementation
   - Species: Probability between 0 and 1 for each species. `runif(n, min = 0, max = 1)` for uniform distribution between 0 and 1 or the Beta distribution to get a skewed distribution between -1 and 1.
   - See `detection.probability` argument in the `sampleOccurrences()` function of the **virtualspecies** package

```{r detection-beta}
# Generate data
n <- 100
x_values <- seq(0, 1, length.out = n)

# Parameters Beta distribution
alpha <- c(0.5, 2, 2, 2, 7, 8)
beta <- c(0.5, 1, 2, 7, 2, 8)

# Calculate densities in dataframe
detection_df <- tibble(
  alpha = alpha,
  beta = beta) %>%
  expand(nesting(alpha, beta), x_values) %>%
  rowwise() %>%
  mutate(
    density = dbeta(x_values, shape1 = alpha, shape2 = beta),
    distribution = paste0("Beta(", alpha, ",", beta, ")")
  )

# Plot the densities
ggplot(detection_df, aes(x = x_values, y = density)) +
  geom_line() +
  labs(x = "Detection probability", y = "Density",
       title = "Densities of different Beta distributions") +
  facet_wrap(~distribution) +
  theme(legend.position = "") +
  theme_minimal()
```

### Sampling effort

-  Sampling effort can be different in *space* and *time*.
-  Values are weights related to number of visits (?)
-  Implementation
   - Space: raster of bias weights to be applied to the sampling of occurrences. Higher weights mean a higher probability of sampling
   - Time: raster can be the same for each time period or differ
   - See `weights` argument in the `sampleOccurrences()` function of **virtualspecies** package

### Spatial uncertainty

-  Spatial uncertainty can be different for each *observation* or the same
-  Values are coordinate uncertainty in meters around observation
-  Implementation
   - Observation: distribution (uniform, (truncated) normal, gamma …) with varying upper bound, can be the same or different for all observations or something in between (extra parameter?)

```{r spat-uncertainty}
# Parameters for the truncated normal distribution
mean_val <- 20
sd_val <- 10

# Parameters for the Gamma distribution
shape <- 2
rate <- 0.1

# Set the lower limit for the truncated distributions
lower_limit <- 0

# Generate x values for the plot
x_values <- seq(lower_limit, mean_val + 5 * sd_val, length.out = 1000)

# Calculate the probability density function (PDF) for the positively truncated
# Normal distribution
density_values_normal <- dnorm(x_values, mean = mean_val, sd = sd_val)

# Calculate the probability density function (PDF) for the positively truncated
# Gamma distribution
density_values_gamma <- dgamma(x_values, shape = shape, rate = rate)

# Create a data frame
coord_uncertainty_df <- tibble(
  x = rep(x_values, 2),
  density = c(density_values_normal, density_values_gamma),
  distribution = rep(c(paste0("Normal", "(", mean_val, ",", sd_val, ")"),
                       paste0("Gamma", "(", shape, ",", rate, ")")),
                     each = length(x_values))
  )

# Plot the positive distributions
ggplot(coord_uncertainty_df, aes(x = x, y = density)) +
  geom_line() +
  labs(x = "Coordinate uncertainty in meters", y = "Density",
       title = "Positive distributions to sample spatial uncertainty") +
  facet_wrap(~distribution) +
  theme_minimal()
```

## Grid designation process

Observation to grid designation is not the focus of this project.
Therefore R code to assign observations to grid cells is already available as a single function.
Here we briefly demonstrate how this function works.

```{r observation-data}
# Set seed for reproducibility
set.seed(123)

# Number of observations and spatial limits
n_points <- 4
xlim <- c(3841000, 3842000)
ylim <- c(3110000, 3112000)

# Create random points
observations_sf <- 
  tibble(
    lat = runif(n_points, ylim[1], ylim[2]),
    long = runif(n_points, xlim[1], xlim[2])
  ) %>%
  st_as_sf(coords = c("long", "lat"), crs = 3035) %>%
  mutate(coordinateUncertaintyInMeters = rgamma(n_points, shape = 5, rate = 0.1))

# Add buffer uncertainty in meters around points
observations_buffered <- observations_sf %>%
  st_buffer(observations_sf$coordinateUncertaintyInMeters)

# Create grid
grid_df <- st_make_grid(
  observations_buffered,
  square = TRUE,
  cellsize = c(200, 200)
  ) %>%
  st_sf() %>%
  mutate(id = row_number())
```

Consider `r n_points` of observations each with a specific coordinate uncertainty.
We can superimpose a grid to create a data cube.

```{r visualise-observations}
ggplot() +
  geom_sf(data = grid_df, linewidth = 1) +
  geom_sf(data = observations_sf, colour = "firebrick") +
  geom_sf(data = observations_buffered, fill = alpha("firebrick", 0.5)) +
  theme_void()
```

The function `grid_designation()` designates observations to cells of a given grid to create an aggregated data cube based on a random point within the uncertainty circle around each observation.
The argument `observations` is an sf object with POINT geometry and a `coordinateUncertaintyInMeters` column.
The first argument `grid` is an sf object with POLYGON geometry (typically a grid) to which observations should be designated.
Further we have the optional argument `id_col` which is the column name of the column with unique ids for each grid cell. If NULL (the default), a column `id` is created were the row numbers represent the unique ids.
With the argument `seed` which you can specify for random number generation to make results reproducible. If NULL (the default), no seed is used.
The function returns an sf object with the number of observations, geometry and minimal coordinate uncertainty per grid cell.

Here, we visualise the output where we see the number of observation per grid cell for this imaginary species in comparison to the original observations (with their uncertainty).
We also show the sampled point within the uncertainty circle (these can be obtained with the argument `aggregate = FALSE`).
If the argument `randomisation = "uniform"` (= default), then each point uncertainty circle has an equal probability to be selected. The other option `randomisation = "normal"` where a point is sampled from a bivariate Normal distribution, but this option is outside the scope of this project.

```{r visualise-cubes}
# Source function to for observation to grid designation 
source(here("R", "grid_designation.R"))

# Create occurrence cube
occurrence_cube_df <- grid_designation(
  observations = observations_sf,
  grid = grid_df,
  id_col = NULL,
  seed = 123)

# Also get sampled points to visualise
sampled_points <- grid_designation(
  observations = observations_sf,
  grid = grid_df,
  id_col = NULL,
  seed = 123,
  aggregate = FALSE)

# Visualise output
ggplot() +
  geom_sf(data = occurrence_cube_df, linewidth = 1) +
  geom_sf_text(data = occurrence_cube_df, aes(label = n)) +
  geom_sf(data = sampled_points, colour = "blue") +
  geom_sf(data = observations_sf, colour = "firebrick") +
  geom_sf(data = observations_buffered, fill = alpha("firebrick", 0.5)) +
  theme_void()
```

# Coding

Here are some ideas

-  A function for each process that can contain multiple helper functions
-  Each function simulates for 1 species, the user can easily simulate multiple species using loops, the `apply()` family functions, ...
-  As input we need a polygon to set the spatial extent or we could create a polygon based on X and Y coordinates given by the user with **sf**?
-  ...?

```{r coding-tab, echo=FALSE}
tibble(
  process = c(
    rep("occurrence", 2),
    rep("detection", 3)
  ),
  variable = c(
    "rarity",
    "spatial clustering",
    "detection probability",
    "sampling effort",
    "spatial uncertainty"
  ),
  default = c(
    "rpois(1, rgamma(1, ?, ?)), where we only simulate for a single time point by default",
    "runif(n, min = -1, max = 1)",
    "1, all occurrences are detected",
    "NULL, default no difference in sampling effort",
    "25 meters?"
  )) %>%
  kable()
```

# Visualisation

At the end of the hackathon it would be nice to have some visual support (and not only lines of code) for the project.
Here are some ideas

-  After the occurrence process, abundances can be plotted over time and/or species distributions can be plotted in space over time.
-  After the detection process, abundances can be plotted over time and/or species distributions can be plotted in space over time.
-  Visualise differences between occurrence process and detection process?
-  After the grid designation process occurrences/abundances can be plotted in space over time.
-  ...?
